{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorya777/Jorya777/blob/main/final_assignment_(UNSDCF_evaluation_dashboard).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyWcaPWe00eg",
        "outputId": "0c2eea26-f095-4b40-fe27-3786cb2467ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit plotly pandas numpy\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb >/dev/null 2>&1 || true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKH_goL1uBP",
        "outputId": "f94fa8d6-5b87-4d16-a481-df3e8e442cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFYxYXXDh_mQ",
        "outputId": "7837975c-aec3-46c8-89fc-2883aefcb804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 python-docx tqdm nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubyyYHCeisyA",
        "outputId": "b77f9930-f36d-49b6-9925-5c6b37c9af46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdfjrTbWqsLl",
        "outputId": "cd9e5d40-ce5b-4c89-d641-6d426feca59e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "100%|██████████| 6/6 [00:43<00:00,  7.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ 完成！共提取 737 条句子。\n",
            "📂 文件已保存至: /content/relevant_sentences_UNSDCF.csv\n",
            "\n",
            "🔍 Sample preview:\n",
            "      Country                                           Sentence  \\\n",
            "0  Azerbaijan  EVALUATION REPORT \\nUNSDCF EVALUATION \\nAzerba...   \n",
            "1  Azerbaijan  Annex 2 Evaluation Design Matrix\\t76\\n3. Annex...   \n",
            "2  Azerbaijan  The document was signed between United Nations...   \n",
            "3  Azerbaijan  Scope and key users: The evaluation covered th...   \n",
            "4  Azerbaijan  The evaluation findings will be utilized to in...   \n",
            "5  Azerbaijan  A contribution analysis was conducted to ident...   \n",
            "6  Azerbaijan  While the CF did not fully repurpose itself to...   \n",
            "7  Azerbaijan  For Outcome 1.1, which focuses on integrating ...   \n",
            "8  Azerbaijan  The reliance on earmarked funding and the need...   \n",
            "9  Azerbaijan  Administrative coordination, while creating ad...   \n",
            "\n",
            "        SourceFile  Sentiment Sentiment_Label        Actor  \n",
            "0  Azerbaijan.docx     0.9360        Positive         UNCT  \n",
            "1  Azerbaijan.docx     0.8807        Positive  Unspecified  \n",
            "2  Azerbaijan.docx     0.9524        Positive         UNCT  \n",
            "3  Azerbaijan.docx    -0.2732        Negative         UNCT  \n",
            "4  Azerbaijan.docx     0.7096        Positive         UNCT  \n",
            "5  Azerbaijan.docx     0.5994        Positive  Unspecified  \n",
            "6  Azerbaijan.docx     0.9451        Positive         UNCT  \n",
            "7  Azerbaijan.docx     0.5859        Positive  Unspecified  \n",
            "8  Azerbaijan.docx     0.3773        Positive         UNCT  \n",
            "9  Azerbaijan.docx     0.8442        Positive  Unspecified  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved: /content/drive/MyDrive/word_frequency_UNSDCF.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# UNSDCF Evaluation Reports Text Extraction\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "\n",
        "# File Path\n",
        "DATA_DIR = \"/content/drive/MyDrive/evaluation_reports\"\n",
        "OUTPUT_CSV = \"/content/relevant_sentences_UNSDCF.csv\"\n",
        "\n",
        "# 该目录中应包含以下文件：\n",
        "# /content/drive/MyDrive/evaluation_reports/Uganda.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Serbia.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Indonesia.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Panama.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Bosnia.docx\n",
        "# /content/drive/MyDrive/evaluation_reports/Azerbaijan.docx\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 🔍 Step 1: 提取文本\n",
        "# ----------------------------------------------------------\n",
        "def extract_text(file_path):\n",
        "    \"\"\"从 PDF 或 DOCX 提取纯文本\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "            reader = PdfReader(file_path)\n",
        "            for page in reader.pages:\n",
        "                t = page.extract_text()\n",
        "                if t:\n",
        "                    text += t + \"\\n\"\n",
        "        elif file_path.endswith(\".docx\"):\n",
        "            doc = Document(file_path)\n",
        "            text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to read {file_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ✂️ Step 2: 提取关键词句\n",
        "# ----------------------------------------------------------\n",
        "KEYWORDS = [\"DCO\", \"RC\", \"UNCT\"]\n",
        "\n",
        "def extract_relevant_sentences(text, country, filename):\n",
        "    \"\"\"提取含有 DCO / RC / UNCT 的句子及上下文\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    data = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        if any(k in s for k in KEYWORDS):\n",
        "            context = \" \".join(sentences[max(0, i-1):min(len(sentences), i+2)])  # 上下文 ±1句\n",
        "            data.append({\n",
        "                \"Country\": country,\n",
        "                \"Sentence\": context.strip(),\n",
        "                \"SourceFile\": filename\n",
        "            })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 🔁 Step 3: 批量读取所有报告\n",
        "# ----------------------------------------------------------\n",
        "all_data = []\n",
        "\n",
        "for file in tqdm(os.listdir(DATA_DIR)):\n",
        "    if not (file.endswith(\".pdf\") or file.endswith(\".docx\")):\n",
        "        continue\n",
        "    file_path = os.path.join(DATA_DIR, file)\n",
        "    # 用文件名提取国家名（如 \"Uganda.pdf\" -> \"Uganda\"）\n",
        "    country = re.sub(r\"[^A-Za-z]\", \" \", os.path.splitext(file)[0]).split()[0]\n",
        "    text = extract_text(file_path)\n",
        "    df = extract_relevant_sentences(text, country, file)\n",
        "    all_data.append(df)\n",
        "\n",
        "df_all = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 💬 Step 4: 情感分析 + 主体识别\n",
        "# ----------------------------------------------------------\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df_all[\"Sentiment\"] = df_all[\"Sentence\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
        "df_all[\"Sentiment_Label\"] = df_all[\"Sentiment\"].apply(\n",
        "    lambda s: \"Positive\" if s > 0.2 else (\"Negative\" if s < -0.2 else \"Neutral\")\n",
        ")\n",
        "\n",
        "def detect_actor(sentence):\n",
        "    actors = []\n",
        "    for a in [\"DCO\", \"RC\", \"UNCT\"]:\n",
        "        if re.search(rf\"\\b{a}\\b\", sentence):\n",
        "            actors.append(a)\n",
        "    return \", \".join(actors) if actors else \"Unspecified\"\n",
        "\n",
        "df_all[\"Actor\"] = df_all[\"Sentence\"].apply(detect_actor)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 💾 Step 5: 保存结果\n",
        "# ----------------------------------------------------------\n",
        "df_all.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"\\n✅ 完成！共提取 {len(df_all)} 条句子。\")\n",
        "print(f\"📂 文件已保存至: {OUTPUT_CSV}\")\n",
        "\n",
        "# 可选：预览前几条\n",
        "print(\"\\n🔍 Sample preview:\")\n",
        "print(df_all.head(10))\n",
        "# ----------------------------------------------------------\n",
        "# ----------------------------------------------------------\n",
        "# 🧠 Step 6: 高频词统计（修复 stop_words 报错 & 更稳健的清洗）\n",
        "# ----------------------------------------------------------\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# 1) 文本预处理：去空值、小写、去多空格\n",
        "def clean_text(s):\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    # 只保留字母和空格（把数字和符号去掉，按需求可调整）\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df_all[\"Sentence_clean\"] = df_all[\"Sentence\"].astype(str).apply(clean_text)\n",
        "\n",
        "# 2) 停用词：用 NLTK 的并转成 list（或直接 stop_words='english' 也行）\n",
        "custom_stop = set(stopwords.words(\"english\"))\n",
        "stop_list = list(custom_stop)\n",
        "\n",
        "# 3) 计算词频\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words=stop_list,      # 关键：不能传 set，要传 list 或 'english'\n",
        "    max_features=1000,\n",
        "    min_df=2                   # 只统计出现≥2次的词（可调）\n",
        ")\n",
        "X = vectorizer.fit_transform(df_all[\"Sentence_clean\"])\n",
        "\n",
        "word_freq = pd.DataFrame({\n",
        "    \"word\": vectorizer.get_feature_names_out(),\n",
        "    \"count\": X.toarray().sum(axis=0)\n",
        "}).sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "# 4) 保存结果文件（供 dashboard 读取）\n",
        "word_freq.to_csv(\"/content/drive/MyDrive/word_frequency_UNSDCF.csv\", index=False)\n",
        "print(\"✅ Saved: /content/drive/MyDrive/word_frequency_UNSDCF.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWJH8x-km_Q9",
        "outputId": "34d4eeb7-e874-439f-f48f-67a2f08defdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# ==========================================================\n",
        "# 📘 UNSDCF Evaluation Dashboard + Text Analysis\n",
        "# ==========================================================\n",
        "st.set_page_config(page_title=\"UNSDCF Evaluation Dashboard\", layout=\"wide\")\n",
        "\n",
        "st.title(\"🌍 United Nations Sustainable Development Cooperation Framework Evaluation Dashboard\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 📊 加载 Evaluation Expenditure 数据\n",
        "# ----------------------------------------------------------\n",
        "file_path = \"/content/drive/MyDrive/2021-2023 evaluation expenditures analysis .xlsx\"\n",
        "df_spend = pd.read_excel(file_path)\n",
        "df_spend.columns = df_spend.columns.str.strip()\n",
        "df_spend.rename(columns={\n",
        "    \"Evaluation expenditure($)\": \"Evaluation Spending ($)\",\n",
        "    \"Program Expenditure\": \"Program Expenditure\",\n",
        "    \"The proportion of Evaluation Expenditure to Program Expenditure\": \"Eval Ratio (%)\"\n",
        "}, inplace=True)\n",
        "for c in [\"Evaluation Spending ($)\", \"Program Expenditure\", \"Eval Ratio (%)\"]:\n",
        "    df_spend[c] = pd.to_numeric(df_spend[c], errors=\"coerce\")\n",
        "df_spend.dropna(subset=[\"Eval Ratio (%)\"], inplace=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 🌍 地图\n",
        "# ----------------------------------------------------------\n",
        "st.subheader(\"🌍 Evaluation Countries (2021–2023)\")\n",
        "fig_map = px.scatter_geo(df_spend, locations=\"Country\", locationmode=\"country names\",\n",
        "                         hover_name=\"Country\", hover_data={\"Evaluation year \": True},\n",
        "                         text=\"Evaluation year \", projection=\"natural earth\")\n",
        "st.plotly_chart(fig_map, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 💰 散点图\n",
        "# ----------------------------------------------------------\n",
        "st.subheader(\"💰 Evaluation vs Programme Expenditure\")\n",
        "fig_scatter = px.scatter(df_spend, x=\"Program Expenditure\", y=\"Eval Ratio (%)\",\n",
        "                         size=\"Evaluation Spending ($)\",\n",
        "                         color=\"Region\" if \"Region\" in df_spend.columns else \"Country\",\n",
        "                         hover_name=\"Country\")\n",
        "st.plotly_chart(fig_scatter, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 📈 雷达图\n",
        "# ----------------------------------------------------------\n",
        "CRITERIA = ['relevance','coherence','effectiveness','efficiency','orientation towards impact','sustainability']\n",
        "countries_eval = [\"Azerbaijan\",\"Uganda\",\"Serbia\",\"Indonesia\",\"Panama\",\"Bosnia and Herzegovina\"]\n",
        "scores = {\n",
        "    \"Azerbaijan\":[4,3,4,3,3,3],\n",
        "    \"Uganda\":[4,2,4,3,3,3],\n",
        "    \"Serbia\":[4,2,4,3,3,3],\n",
        "    \"Indonesia\":[5,3,4,3,4,3],\n",
        "    \"Panama\":[4,3,3,3,3,2],\n",
        "    \"Bosnia and Herzegovina\":[4,2,4,3,3,3]\n",
        "}\n",
        "df_scores = pd.DataFrame([{\"Country\":c,\"Criterion\":crit,\"Score\":scores[c][i]}\n",
        "                          for c in countries_eval for i,crit in enumerate(CRITERIA)])\n",
        "country = st.sidebar.selectbox(\"Select Country\", countries_eval)\n",
        "fig_radar = px.line_polar(df_scores[df_scores[\"Country\"]==country],\n",
        "                          r=\"Score\", theta=\"Criterion\", line_close=True)\n",
        "st.plotly_chart(fig_radar, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 文本分析结果整合\n",
        "# ----------------------------------------------------------\n",
        "st.header(\" Text Analysis: Mentions of DCO / RC / UNCT\")\n",
        "\n",
        "try:\n",
        "    df_mentions = pd.read_csv(\"/content/drive/MyDrive/relevant_sentences_UNSDCF_scored.csv\")\n",
        "    df_words = pd.read_csv(\"/content/drive/MyDrive/word_frequency_UNSDCF.csv\")\n",
        "\n",
        "    st.subheader(\"📑 Sample Extracted Mentions\")\n",
        "    st.dataframe(df_mentions.head(10))\n",
        "\n",
        "    st.subheader(\"📊 Sentiment Distribution\")\n",
        "    sent_summary = df_mentions.groupby(\"Sentiment_Label\").size().reset_index(name=\"Count\")\n",
        "    fig_sent = px.bar(sent_summary, x=\"Sentiment_Label\", y=\"Count\", color=\"Sentiment_Label\")\n",
        "    st.plotly_chart(fig_sent, use_container_width=True)\n",
        "\n",
        "    st.subheader(\"🔤 Top Keywords in Mentions\")\n",
        "    top_words = df_words.head(20)\n",
        "    fig_words = px.bar(top_words, x=\"word\", y=\"count\", title=\"Top 20 Words in DCO/RC/UNCT Mentions\", color=\"count\")\n",
        "    st.plotly_chart(fig_words, use_container_width=True)\n",
        "\n",
        "except Exception as e:\n",
        "    st.warning(f\"⚠️ Text analysis results not found or failed to load: {e}\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"© United Nations DCO – Data visualization for learning purposes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time, re\n",
        "\n",
        "PORT = \"8501\"\n",
        "\n",
        "# 启动 Streamlit\n",
        "streamlit = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", PORT, \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# 启动 Cloudflare Tunnel\n",
        "tunnel = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{PORT}\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "\n",
        "for line in tunnel.stdout:\n",
        "    print(line.strip())\n",
        "    m = re.search(r\"https://[a-z0-9-]+\\.trycloudflare\\.com\", line)\n",
        "    if m:\n",
        "        print(\"\\n🚀 Streamlit app URL:\", m.group(0))\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H90tBTse7QAx",
        "outputId": "c90401be-d00f-49e2-88c2-c61ee03d52a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Cloudflare Tunnel...\n",
            "2025-10-25T19:51:33Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:51:33Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:51:36Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:51:36Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:51:36Z INF |  https://mandate-senate-protective-furnished.trycloudflare.com                             |\n",
            "\n",
            "🚀 Streamlit app URL: https://mandate-senate-protective-furnished.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# 确保 cloudflared 已安装\n",
        "!which cloudflared || (wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb)\n",
        "\n",
        "# 启动 Streamlit 的函数\n",
        "def run_streamlit():\n",
        "    subprocess.run([\n",
        "        \"streamlit\", \"run\", \"/content/app.py\",\n",
        "        \"--server.port\", \"8501\",\n",
        "        \"--server.headless\", \"true\",\n",
        "        \"--server.enableCORS\", \"false\",\n",
        "        \"--server.enableXsrfProtection\", \"false\"\n",
        "    ])\n",
        "\n",
        "# 在后台启动 Streamlit\n",
        "print(\"🚀 启动 Streamlit 服务器...\")\n",
        "streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "streamlit_thread.start()\n",
        "\n",
        "# 等待 Streamlit 启动\n",
        "print(\"⏳ 等待 Streamlit 启动...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 启动 Cloudflare Tunnel - 使用更稳定的方法\n",
        "print(\"🌐 启动 Cloudflare Tunnel...\")\n",
        "tunnel_process = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "# 获取公共 URL\n",
        "print(\"🔗 获取公共访问链接...\")\n",
        "for i in range(30):  # 最多等待30秒\n",
        "    line = tunnel_process.stdout.readline()\n",
        "    if not line:\n",
        "        time.sleep(1)\n",
        "        continue\n",
        "    print(line.strip())\n",
        "    if \".trycloudflare.com\" in line:\n",
        "        url_match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', line)\n",
        "        if url_match:\n",
        "            public_url = url_match.group(0)\n",
        "            print(f\"\\n🎉 你的应用已经上线了！\")\n",
        "            print(f\"👉 访问地址: {public_url}\")\n",
        "            print(\"💡 注意：保持这个单元格运行，你的应用才能持续被访问。\")\n",
        "            break\n",
        "else:\n",
        "    print(\"❌ 未能获取公共URL，请检查 cloudflared 输出信息\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNhochab5aMF",
        "outputId": "a99496c6-bf09-4a5a-8dc2-3f2cf107f902"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/cloudflared\n",
            "🚀 启动 Streamlit 服务器...\n",
            "⏳ 等待 Streamlit 启动...\n",
            "🌐 启动 Cloudflare Tunnel...\n",
            "🔗 获取公共访问链接...\n",
            "2025-10-25T19:43:35Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:43:35Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:43:38Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:43:38Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:43:38Z INF |  https://freedom-unity-compatible-hugo.trycloudflare.com                                   |\n",
            "\n",
            "🎉 你的应用已经上线了！\n",
            "👉 访问地址: https://freedom-unity-compatible-hugo.trycloudflare.com\n",
            "💡 注意：保持这个单元格运行，你的应用才能持续被访问。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok -q\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# 设置你的ngrok authtoken\n",
        "NGROK_AUTHTOKEN = \"34ZUqhGK4EnIjB6UnPlPWnMswTN_6XqUWB7CXMfhaX8XSBy2x\"  # 请务必替换！\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# 启动Streamlit的函数\n",
        "def run_streamlit():\n",
        "    subprocess.run([\"streamlit\", \"run\", \"/content/app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"])\n",
        "\n",
        "# 在后台启动Streamlit\n",
        "print(\"🚀 启动Streamlit服务器...\")\n",
        "streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "streamlit_thread.start()\n",
        "\n",
        "# 等待一下确保Streamlit启动\n",
        "time.sleep(5)\n",
        "\n",
        "# 建立隧道\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(f\"\\n🎉 你的应用已经上线了！\")\n",
        "print(f\"👉 访问地址: {public_url}\")\n",
        "print(\"💡 注意：保持这个单元格运行，你的应用才能持续被访问。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofkzW_a24UbX",
        "outputId": "8d2c3129-13e0-4b18-f7ff-5ea64e9e20ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 启动Streamlit服务器...\n",
            "\n",
            "🎉 你的应用已经上线了！\n",
            "👉 访问地址: NgrokTunnel: \"https://unnocturnally-undazing-sylvia.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "💡 注意：保持这个单元格运行，你的应用才能持续被访问。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHi0VMv1atS",
        "outputId": "637e4f67-34ce-4e39-9989-aed086b69aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Cloudflare Tunnel … (keep this cell running)\n",
            "2025-10-25T19:31:52Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:31:52Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:31:55Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:31:55Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:31:55Z INF |  https://priest-afterwards-red-concerned.trycloudflare.com                                 |\n",
            "\n",
            "🚀 Streamlit app URL: https://priest-afterwards-red-concerned.trycloudflare.com\n",
            "No password required. Keep this cell running while you use the app.\n"
          ]
        }
      ],
      "source": [
        "import subprocess, time, re, sys\n",
        "\n",
        "PORT = \"8501\"\n",
        "# 启动 Streamlit 后台服务\n",
        "streamlit = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", PORT, \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "time.sleep(2)\n",
        "\n",
        "# 启动 Cloudflare Tunnel\n",
        "tunnel = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{PORT}\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "print(\"Starting Cloudflare Tunnel … (keep this cell running)\")\n",
        "for line in tunnel.stdout:\n",
        "    sys.stdout.write(line)\n",
        "    sys.stdout.flush()\n",
        "    m = re.search(r\"https://[a-z0-9-]+\\.trycloudflare\\.com\", line)\n",
        "    if m:\n",
        "        print(\"\\n🚀 Streamlit app URL:\", m.group(0))\n",
        "        print(\"No password required. Keep this cell running while you use the app.\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/CjXsKw++FgCVOhNdQr7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}