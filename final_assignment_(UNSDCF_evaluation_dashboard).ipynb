{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jorya777/Jorya777/blob/main/final_assignment_(UNSDCF_evaluation_dashboard).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyWcaPWe00eg",
        "outputId": "0c2eea26-f095-4b40-fe27-3786cb2467ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit plotly pandas numpy\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb >/dev/null 2>&1 || true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKH_goL1uBP",
        "outputId": "f94fa8d6-5b87-4d16-a481-df3e8e442cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFYxYXXDh_mQ",
        "outputId": "7837975c-aec3-46c8-89fc-2883aefcb804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 python-docx tqdm nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubyyYHCeisyA",
        "outputId": "b77f9930-f36d-49b6-9925-5c6b37c9af46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdfjrTbWqsLl",
        "outputId": "cd9e5d40-ce5b-4c89-d641-6d426feca59e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:43<00:00,  7.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… å®Œæˆï¼å…±æå– 737 æ¡å¥å­ã€‚\n",
            "ğŸ“‚ æ–‡ä»¶å·²ä¿å­˜è‡³: /content/relevant_sentences_UNSDCF.csv\n",
            "\n",
            "ğŸ” Sample preview:\n",
            "      Country                                           Sentence  \\\n",
            "0  Azerbaijan  EVALUATION REPORT \\nUNSDCF EVALUATION \\nAzerba...   \n",
            "1  Azerbaijan  Annex 2 Evaluation Design Matrix\\t76\\n3. Annex...   \n",
            "2  Azerbaijan  The document was signed between United Nations...   \n",
            "3  Azerbaijan  Scope and key users: The evaluation covered th...   \n",
            "4  Azerbaijan  The evaluation findings will be utilized to in...   \n",
            "5  Azerbaijan  A contribution analysis was conducted to ident...   \n",
            "6  Azerbaijan  While the CF did not fully repurpose itself to...   \n",
            "7  Azerbaijan  For Outcome 1.1, which focuses on integrating ...   \n",
            "8  Azerbaijan  The reliance on earmarked funding and the need...   \n",
            "9  Azerbaijan  Administrative coordination, while creating ad...   \n",
            "\n",
            "        SourceFile  Sentiment Sentiment_Label        Actor  \n",
            "0  Azerbaijan.docx     0.9360        Positive         UNCT  \n",
            "1  Azerbaijan.docx     0.8807        Positive  Unspecified  \n",
            "2  Azerbaijan.docx     0.9524        Positive         UNCT  \n",
            "3  Azerbaijan.docx    -0.2732        Negative         UNCT  \n",
            "4  Azerbaijan.docx     0.7096        Positive         UNCT  \n",
            "5  Azerbaijan.docx     0.5994        Positive  Unspecified  \n",
            "6  Azerbaijan.docx     0.9451        Positive         UNCT  \n",
            "7  Azerbaijan.docx     0.5859        Positive  Unspecified  \n",
            "8  Azerbaijan.docx     0.3773        Positive         UNCT  \n",
            "9  Azerbaijan.docx     0.8442        Positive  Unspecified  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Saved: /content/drive/MyDrive/word_frequency_UNSDCF.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# UNSDCF Evaluation Reports Text Extraction\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "\n",
        "# File Path\n",
        "DATA_DIR = \"/content/drive/MyDrive/evaluation_reports\"\n",
        "OUTPUT_CSV = \"/content/relevant_sentences_UNSDCF.csv\"\n",
        "\n",
        "# è¯¥ç›®å½•ä¸­åº”åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š\n",
        "# /content/drive/MyDrive/evaluation_reports/Uganda.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Serbia.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Indonesia.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Panama.pdf\n",
        "# /content/drive/MyDrive/evaluation_reports/Bosnia.docx\n",
        "# /content/drive/MyDrive/evaluation_reports/Azerbaijan.docx\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ” Step 1: æå–æ–‡æœ¬\n",
        "# ----------------------------------------------------------\n",
        "def extract_text(file_path):\n",
        "    \"\"\"ä» PDF æˆ– DOCX æå–çº¯æ–‡æœ¬\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "            reader = PdfReader(file_path)\n",
        "            for page in reader.pages:\n",
        "                t = page.extract_text()\n",
        "                if t:\n",
        "                    text += t + \"\\n\"\n",
        "        elif file_path.endswith(\".docx\"):\n",
        "            doc = Document(file_path)\n",
        "            text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed to read {file_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# âœ‚ï¸ Step 2: æå–å…³é”®è¯å¥\n",
        "# ----------------------------------------------------------\n",
        "KEYWORDS = [\"DCO\", \"RC\", \"UNCT\"]\n",
        "\n",
        "def extract_relevant_sentences(text, country, filename):\n",
        "    \"\"\"æå–å«æœ‰ DCO / RC / UNCT çš„å¥å­åŠä¸Šä¸‹æ–‡\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    data = []\n",
        "    for i, s in enumerate(sentences):\n",
        "        if any(k in s for k in KEYWORDS):\n",
        "            context = \" \".join(sentences[max(0, i-1):min(len(sentences), i+2)])  # ä¸Šä¸‹æ–‡ Â±1å¥\n",
        "            data.append({\n",
        "                \"Country\": country,\n",
        "                \"Sentence\": context.strip(),\n",
        "                \"SourceFile\": filename\n",
        "            })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ” Step 3: æ‰¹é‡è¯»å–æ‰€æœ‰æŠ¥å‘Š\n",
        "# ----------------------------------------------------------\n",
        "all_data = []\n",
        "\n",
        "for file in tqdm(os.listdir(DATA_DIR)):\n",
        "    if not (file.endswith(\".pdf\") or file.endswith(\".docx\")):\n",
        "        continue\n",
        "    file_path = os.path.join(DATA_DIR, file)\n",
        "    # ç”¨æ–‡ä»¶åæå–å›½å®¶åï¼ˆå¦‚ \"Uganda.pdf\" -> \"Uganda\"ï¼‰\n",
        "    country = re.sub(r\"[^A-Za-z]\", \" \", os.path.splitext(file)[0]).split()[0]\n",
        "    text = extract_text(file_path)\n",
        "    df = extract_relevant_sentences(text, country, file)\n",
        "    all_data.append(df)\n",
        "\n",
        "df_all = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ’¬ Step 4: æƒ…æ„Ÿåˆ†æ + ä¸»ä½“è¯†åˆ«\n",
        "# ----------------------------------------------------------\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "df_all[\"Sentiment\"] = df_all[\"Sentence\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
        "df_all[\"Sentiment_Label\"] = df_all[\"Sentiment\"].apply(\n",
        "    lambda s: \"Positive\" if s > 0.2 else (\"Negative\" if s < -0.2 else \"Neutral\")\n",
        ")\n",
        "\n",
        "def detect_actor(sentence):\n",
        "    actors = []\n",
        "    for a in [\"DCO\", \"RC\", \"UNCT\"]:\n",
        "        if re.search(rf\"\\b{a}\\b\", sentence):\n",
        "            actors.append(a)\n",
        "    return \", \".join(actors) if actors else \"Unspecified\"\n",
        "\n",
        "df_all[\"Actor\"] = df_all[\"Sentence\"].apply(detect_actor)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ’¾ Step 5: ä¿å­˜ç»“æœ\n",
        "# ----------------------------------------------------------\n",
        "df_all.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"\\nâœ… å®Œæˆï¼å…±æå– {len(df_all)} æ¡å¥å­ã€‚\")\n",
        "print(f\"ğŸ“‚ æ–‡ä»¶å·²ä¿å­˜è‡³: {OUTPUT_CSV}\")\n",
        "\n",
        "# å¯é€‰ï¼šé¢„è§ˆå‰å‡ æ¡\n",
        "print(\"\\nğŸ” Sample preview:\")\n",
        "print(df_all.head(10))\n",
        "# ----------------------------------------------------------\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ§  Step 6: é«˜é¢‘è¯ç»Ÿè®¡ï¼ˆä¿®å¤ stop_words æŠ¥é”™ & æ›´ç¨³å¥çš„æ¸…æ´—ï¼‰\n",
        "# ----------------------------------------------------------\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# 1) æ–‡æœ¬é¢„å¤„ç†ï¼šå»ç©ºå€¼ã€å°å†™ã€å»å¤šç©ºæ ¼\n",
        "def clean_text(s):\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    # åªä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼ˆæŠŠæ•°å­—å’Œç¬¦å·å»æ‰ï¼ŒæŒ‰éœ€æ±‚å¯è°ƒæ•´ï¼‰\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df_all[\"Sentence_clean\"] = df_all[\"Sentence\"].astype(str).apply(clean_text)\n",
        "\n",
        "# 2) åœç”¨è¯ï¼šç”¨ NLTK çš„å¹¶è½¬æˆ listï¼ˆæˆ–ç›´æ¥ stop_words='english' ä¹Ÿè¡Œï¼‰\n",
        "custom_stop = set(stopwords.words(\"english\"))\n",
        "stop_list = list(custom_stop)\n",
        "\n",
        "# 3) è®¡ç®—è¯é¢‘\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words=stop_list,      # å…³é”®ï¼šä¸èƒ½ä¼  setï¼Œè¦ä¼  list æˆ– 'english'\n",
        "    max_features=1000,\n",
        "    min_df=2                   # åªç»Ÿè®¡å‡ºç°â‰¥2æ¬¡çš„è¯ï¼ˆå¯è°ƒï¼‰\n",
        ")\n",
        "X = vectorizer.fit_transform(df_all[\"Sentence_clean\"])\n",
        "\n",
        "word_freq = pd.DataFrame({\n",
        "    \"word\": vectorizer.get_feature_names_out(),\n",
        "    \"count\": X.toarray().sum(axis=0)\n",
        "}).sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "# 4) ä¿å­˜ç»“æœæ–‡ä»¶ï¼ˆä¾› dashboard è¯»å–ï¼‰\n",
        "word_freq.to_csv(\"/content/drive/MyDrive/word_frequency_UNSDCF.csv\", index=False)\n",
        "print(\"âœ… Saved: /content/drive/MyDrive/word_frequency_UNSDCF.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWJH8x-km_Q9",
        "outputId": "34d4eeb7-e874-439f-f48f-67a2f08defdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/drive/MyDrive/app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# ==========================================================\n",
        "# ğŸ“˜ UNSDCF Evaluation Dashboard + Text Analysis\n",
        "# ==========================================================\n",
        "st.set_page_config(page_title=\"UNSDCF Evaluation Dashboard\", layout=\"wide\")\n",
        "\n",
        "st.title(\"ğŸŒ United Nations Sustainable Development Cooperation Framework Evaluation Dashboard\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ“Š åŠ è½½ Evaluation Expenditure æ•°æ®\n",
        "# ----------------------------------------------------------\n",
        "file_path = \"/content/drive/MyDrive/2021-2023 evaluation expenditures analysis .xlsx\"\n",
        "df_spend = pd.read_excel(file_path)\n",
        "df_spend.columns = df_spend.columns.str.strip()\n",
        "df_spend.rename(columns={\n",
        "    \"Evaluation expenditure($)\": \"Evaluation Spending ($)\",\n",
        "    \"Program Expenditure\": \"Program Expenditure\",\n",
        "    \"The proportion of Evaluation Expenditure to Program Expenditure\": \"Eval Ratio (%)\"\n",
        "}, inplace=True)\n",
        "for c in [\"Evaluation Spending ($)\", \"Program Expenditure\", \"Eval Ratio (%)\"]:\n",
        "    df_spend[c] = pd.to_numeric(df_spend[c], errors=\"coerce\")\n",
        "df_spend.dropna(subset=[\"Eval Ratio (%)\"], inplace=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸŒ åœ°å›¾\n",
        "# ----------------------------------------------------------\n",
        "st.subheader(\"ğŸŒ Evaluation Countries (2021â€“2023)\")\n",
        "fig_map = px.scatter_geo(df_spend, locations=\"Country\", locationmode=\"country names\",\n",
        "                         hover_name=\"Country\", hover_data={\"Evaluation year \": True},\n",
        "                         text=\"Evaluation year \", projection=\"natural earth\")\n",
        "st.plotly_chart(fig_map, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ’° æ•£ç‚¹å›¾\n",
        "# ----------------------------------------------------------\n",
        "st.subheader(\"ğŸ’° Evaluation vs Programme Expenditure\")\n",
        "fig_scatter = px.scatter(df_spend, x=\"Program Expenditure\", y=\"Eval Ratio (%)\",\n",
        "                         size=\"Evaluation Spending ($)\",\n",
        "                         color=\"Region\" if \"Region\" in df_spend.columns else \"Country\",\n",
        "                         hover_name=\"Country\")\n",
        "st.plotly_chart(fig_scatter, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ğŸ“ˆ é›·è¾¾å›¾\n",
        "# ----------------------------------------------------------\n",
        "CRITERIA = ['relevance','coherence','effectiveness','efficiency','orientation towards impact','sustainability']\n",
        "countries_eval = [\"Azerbaijan\",\"Uganda\",\"Serbia\",\"Indonesia\",\"Panama\",\"Bosnia and Herzegovina\"]\n",
        "scores = {\n",
        "    \"Azerbaijan\":[4,3,4,3,3,3],\n",
        "    \"Uganda\":[4,2,4,3,3,3],\n",
        "    \"Serbia\":[4,2,4,3,3,3],\n",
        "    \"Indonesia\":[5,3,4,3,4,3],\n",
        "    \"Panama\":[4,3,3,3,3,2],\n",
        "    \"Bosnia and Herzegovina\":[4,2,4,3,3,3]\n",
        "}\n",
        "df_scores = pd.DataFrame([{\"Country\":c,\"Criterion\":crit,\"Score\":scores[c][i]}\n",
        "                          for c in countries_eval for i,crit in enumerate(CRITERIA)])\n",
        "country = st.sidebar.selectbox(\"Select Country\", countries_eval)\n",
        "fig_radar = px.line_polar(df_scores[df_scores[\"Country\"]==country],\n",
        "                          r=\"Score\", theta=\"Criterion\", line_close=True)\n",
        "st.plotly_chart(fig_radar, use_container_width=True)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# æ–‡æœ¬åˆ†æç»“æœæ•´åˆ\n",
        "# ----------------------------------------------------------\n",
        "st.header(\" Text Analysis: Mentions of DCO / RC / UNCT\")\n",
        "\n",
        "try:\n",
        "    df_mentions = pd.read_csv(\"/content/drive/MyDrive/relevant_sentences_UNSDCF_scored.csv\")\n",
        "    df_words = pd.read_csv(\"/content/drive/MyDrive/word_frequency_UNSDCF.csv\")\n",
        "\n",
        "    st.subheader(\"ğŸ“‘ Sample Extracted Mentions\")\n",
        "    st.dataframe(df_mentions.head(10))\n",
        "\n",
        "    st.subheader(\"ğŸ“Š Sentiment Distribution\")\n",
        "    sent_summary = df_mentions.groupby(\"Sentiment_Label\").size().reset_index(name=\"Count\")\n",
        "    fig_sent = px.bar(sent_summary, x=\"Sentiment_Label\", y=\"Count\", color=\"Sentiment_Label\")\n",
        "    st.plotly_chart(fig_sent, use_container_width=True)\n",
        "\n",
        "    st.subheader(\"ğŸ”¤ Top Keywords in Mentions\")\n",
        "    top_words = df_words.head(20)\n",
        "    fig_words = px.bar(top_words, x=\"word\", y=\"count\", title=\"Top 20 Words in DCO/RC/UNCT Mentions\", color=\"count\")\n",
        "    st.plotly_chart(fig_words, use_container_width=True)\n",
        "\n",
        "except Exception as e:\n",
        "    st.warning(f\"âš ï¸ Text analysis results not found or failed to load: {e}\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Â© United Nations DCO â€“ Data visualization for learning purposes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, time, re\n",
        "\n",
        "PORT = \"8501\"\n",
        "\n",
        "# å¯åŠ¨ Streamlit\n",
        "streamlit = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", PORT, \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "time.sleep(5)\n",
        "\n",
        "# å¯åŠ¨ Cloudflare Tunnel\n",
        "tunnel = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{PORT}\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "\n",
        "for line in tunnel.stdout:\n",
        "    print(line.strip())\n",
        "    m = re.search(r\"https://[a-z0-9-]+\\.trycloudflare\\.com\", line)\n",
        "    if m:\n",
        "        print(\"\\nğŸš€ Streamlit app URL:\", m.group(0))\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H90tBTse7QAx",
        "outputId": "c90401be-d00f-49e2-88c2-c61ee03d52a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Cloudflare Tunnel...\n",
            "2025-10-25T19:51:33Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:51:33Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:51:36Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:51:36Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:51:36Z INF |  https://mandate-senate-protective-furnished.trycloudflare.com                             |\n",
            "\n",
            "ğŸš€ Streamlit app URL: https://mandate-senate-protective-furnished.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# ç¡®ä¿ cloudflared å·²å®‰è£…\n",
        "!which cloudflared || (wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb)\n",
        "\n",
        "# å¯åŠ¨ Streamlit çš„å‡½æ•°\n",
        "def run_streamlit():\n",
        "    subprocess.run([\n",
        "        \"streamlit\", \"run\", \"/content/app.py\",\n",
        "        \"--server.port\", \"8501\",\n",
        "        \"--server.headless\", \"true\",\n",
        "        \"--server.enableCORS\", \"false\",\n",
        "        \"--server.enableXsrfProtection\", \"false\"\n",
        "    ])\n",
        "\n",
        "# åœ¨åå°å¯åŠ¨ Streamlit\n",
        "print(\"ğŸš€ å¯åŠ¨ Streamlit æœåŠ¡å™¨...\")\n",
        "streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "streamlit_thread.start()\n",
        "\n",
        "# ç­‰å¾… Streamlit å¯åŠ¨\n",
        "print(\"â³ ç­‰å¾… Streamlit å¯åŠ¨...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# å¯åŠ¨ Cloudflare Tunnel - ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹æ³•\n",
        "print(\"ğŸŒ å¯åŠ¨ Cloudflare Tunnel...\")\n",
        "tunnel_process = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "# è·å–å…¬å…± URL\n",
        "print(\"ğŸ”— è·å–å…¬å…±è®¿é—®é“¾æ¥...\")\n",
        "for i in range(30):  # æœ€å¤šç­‰å¾…30ç§’\n",
        "    line = tunnel_process.stdout.readline()\n",
        "    if not line:\n",
        "        time.sleep(1)\n",
        "        continue\n",
        "    print(line.strip())\n",
        "    if \".trycloudflare.com\" in line:\n",
        "        url_match = re.search(r'https://[a-zA-Z0-9-]+\\.trycloudflare\\.com', line)\n",
        "        if url_match:\n",
        "            public_url = url_match.group(0)\n",
        "            print(f\"\\nğŸ‰ ä½ çš„åº”ç”¨å·²ç»ä¸Šçº¿äº†ï¼\")\n",
        "            print(f\"ğŸ‘‰ è®¿é—®åœ°å€: {public_url}\")\n",
        "            print(\"ğŸ’¡ æ³¨æ„ï¼šä¿æŒè¿™ä¸ªå•å…ƒæ ¼è¿è¡Œï¼Œä½ çš„åº”ç”¨æ‰èƒ½æŒç»­è¢«è®¿é—®ã€‚\")\n",
        "            break\n",
        "else:\n",
        "    print(\"âŒ æœªèƒ½è·å–å…¬å…±URLï¼Œè¯·æ£€æŸ¥ cloudflared è¾“å‡ºä¿¡æ¯\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNhochab5aMF",
        "outputId": "a99496c6-bf09-4a5a-8dc2-3f2cf107f902"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/cloudflared\n",
            "ğŸš€ å¯åŠ¨ Streamlit æœåŠ¡å™¨...\n",
            "â³ ç­‰å¾… Streamlit å¯åŠ¨...\n",
            "ğŸŒ å¯åŠ¨ Cloudflare Tunnel...\n",
            "ğŸ”— è·å–å…¬å…±è®¿é—®é“¾æ¥...\n",
            "2025-10-25T19:43:35Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:43:35Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:43:38Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:43:38Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:43:38Z INF |  https://freedom-unity-compatible-hugo.trycloudflare.com                                   |\n",
            "\n",
            "ğŸ‰ ä½ çš„åº”ç”¨å·²ç»ä¸Šçº¿äº†ï¼\n",
            "ğŸ‘‰ è®¿é—®åœ°å€: https://freedom-unity-compatible-hugo.trycloudflare.com\n",
            "ğŸ’¡ æ³¨æ„ï¼šä¿æŒè¿™ä¸ªå•å…ƒæ ¼è¿è¡Œï¼Œä½ çš„åº”ç”¨æ‰èƒ½æŒç»­è¢«è®¿é—®ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok -q\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# è®¾ç½®ä½ çš„ngrok authtoken\n",
        "NGROK_AUTHTOKEN = \"34ZUqhGK4EnIjB6UnPlPWnMswTN_6XqUWB7CXMfhaX8XSBy2x\"  # è¯·åŠ¡å¿…æ›¿æ¢ï¼\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# å¯åŠ¨Streamlitçš„å‡½æ•°\n",
        "def run_streamlit():\n",
        "    subprocess.run([\"streamlit\", \"run\", \"/content/app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"])\n",
        "\n",
        "# åœ¨åå°å¯åŠ¨Streamlit\n",
        "print(\"ğŸš€ å¯åŠ¨StreamlitæœåŠ¡å™¨...\")\n",
        "streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "streamlit_thread.start()\n",
        "\n",
        "# ç­‰å¾…ä¸€ä¸‹ç¡®ä¿Streamlitå¯åŠ¨\n",
        "time.sleep(5)\n",
        "\n",
        "# å»ºç«‹éš§é“\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(f\"\\nğŸ‰ ä½ çš„åº”ç”¨å·²ç»ä¸Šçº¿äº†ï¼\")\n",
        "print(f\"ğŸ‘‰ è®¿é—®åœ°å€: {public_url}\")\n",
        "print(\"ğŸ’¡ æ³¨æ„ï¼šä¿æŒè¿™ä¸ªå•å…ƒæ ¼è¿è¡Œï¼Œä½ çš„åº”ç”¨æ‰èƒ½æŒç»­è¢«è®¿é—®ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofkzW_a24UbX",
        "outputId": "8d2c3129-13e0-4b18-f7ff-5ea64e9e20ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å¯åŠ¨StreamlitæœåŠ¡å™¨...\n",
            "\n",
            "ğŸ‰ ä½ çš„åº”ç”¨å·²ç»ä¸Šçº¿äº†ï¼\n",
            "ğŸ‘‰ è®¿é—®åœ°å€: NgrokTunnel: \"https://unnocturnally-undazing-sylvia.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "ğŸ’¡ æ³¨æ„ï¼šä¿æŒè¿™ä¸ªå•å…ƒæ ¼è¿è¡Œï¼Œä½ çš„åº”ç”¨æ‰èƒ½æŒç»­è¢«è®¿é—®ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHi0VMv1atS",
        "outputId": "637e4f67-34ce-4e39-9989-aed086b69aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Cloudflare Tunnel â€¦ (keep this cell running)\n",
            "2025-10-25T19:31:52Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-25T19:31:52Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-25T19:31:55Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-25T19:31:55Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-25T19:31:55Z INF |  https://priest-afterwards-red-concerned.trycloudflare.com                                 |\n",
            "\n",
            "ğŸš€ Streamlit app URL: https://priest-afterwards-red-concerned.trycloudflare.com\n",
            "No password required. Keep this cell running while you use the app.\n"
          ]
        }
      ],
      "source": [
        "import subprocess, time, re, sys\n",
        "\n",
        "PORT = \"8501\"\n",
        "# å¯åŠ¨ Streamlit åå°æœåŠ¡\n",
        "streamlit = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", PORT, \"--server.headless\", \"true\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "time.sleep(2)\n",
        "\n",
        "# å¯åŠ¨ Cloudflare Tunnel\n",
        "tunnel = subprocess.Popen(\n",
        "    [\"cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{PORT}\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "print(\"Starting Cloudflare Tunnel â€¦ (keep this cell running)\")\n",
        "for line in tunnel.stdout:\n",
        "    sys.stdout.write(line)\n",
        "    sys.stdout.flush()\n",
        "    m = re.search(r\"https://[a-z0-9-]+\\.trycloudflare\\.com\", line)\n",
        "    if m:\n",
        "        print(\"\\nğŸš€ Streamlit app URL:\", m.group(0))\n",
        "        print(\"No password required. Keep this cell running while you use the app.\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/CjXsKw++FgCVOhNdQr7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}